#!/usr/bin/env python3
"""
·ª®ng d·ª•ng ch√≠nh cho ph√¢n t√≠ch bi·∫øn ƒë·ªông s·ª≠ d·ª•ng ƒë·∫•t/l·ªõp ph·ªß b·ªÅ m·∫∑t ƒë·∫•t t·ªânh ƒê·ªìng Th√°p

Author: AI Assistant
Date: 2024
"""

import argparse
import logging
import yaml
from pathlib import Path
import sys
import os
from typing import Dict, List

# Th√™m src v√†o path ƒë·ªÉ import modules
sys.path.append(str(Path(__file__).parent / "src"))

from data_processing.gee_client import GEEClient
from data_processing.preprocessor import SatelliteDataPreprocessor
from models.ml_models import RandomForestModel, SVMModel, CNNModel, ModelEvaluator
from analysis.change_detection import LandUseChangeAnalyzer
from visualization.map_visualizer import MapVisualizer

class DongThapLandChangeApp:
    """·ª®ng d·ª•ng ch√≠nh cho ph√¢n t√≠ch bi·∫øn ƒë·ªông s·ª≠ d·ª•ng ƒë·∫•t t·ªânh ƒê·ªìng Th√°p"""
    
    def __init__(self, config_path: str = "config/config.yaml"):
        """
        Kh·ªüi t·∫°o ·ª©ng d·ª•ng
        
        Args:
            config_path: ƒê∆∞·ªùng d·∫´n file c·∫•u h√¨nh
        """
        self.config_path = config_path
        self.config = self._load_config()
        self.logger = self._setup_logger()
        self.validation_mode = False  # Ch·∫ø ƒë·ªô validation chi ti·∫øt
        
        # Kh·ªüi t·∫°o c√°c components
        self.gee_client = None
        self.preprocessor = None
        self.models = {}
        self.change_analyzer = None
        self.visualizer = None
        
        self._initialize_components()
        
    def _load_config(self) -> Dict:
        """ƒê·ªçc file c·∫•u h√¨nh"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except FileNotFoundError:
            print(f"[ERROR] Kh√¥ng t√¨m th·∫•y file c·∫•u h√¨nh: {self.config_path}")
            sys.exit(1)
    
    def _setup_logger(self) -> logging.Logger:
        """Thi·∫øt l·∫≠p logging"""
        log_dir = Path(self.config['paths']['log_dir'])
        log_dir.mkdir(exist_ok=True)
        
        # T·∫°o handlers v·ªõi UTF-8 encoding
        file_handler = logging.FileHandler(
            log_dir / 'dong_thap_analysis.log', 
            encoding='utf-8'
        )
        
        # Console handler v·ªõi fallback cho Windows
        console_handler = logging.StreamHandler()
        
        # Thi·∫øt l·∫≠p format
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        file_handler.setFormatter(formatter)
        console_handler.setFormatter(formatter)
        
        # T·∫°o logger
        logger = logging.getLogger(__name__)
        logger.setLevel(logging.INFO)
        
        # Th√™m handlers n·∫øu ch∆∞a c√≥
        if not logger.handlers:
            logger.addHandler(file_handler)
            logger.addHandler(console_handler)
        
        return logger
    
    def _initialize_components(self):
        """Kh·ªüi t·∫°o c√°c components"""
        try:
            self.logger.info("[INIT] Kh·ªüi t·∫°o ·ª©ng d·ª•ng Ph√¢n t√≠ch Bi·∫øn ƒë·ªông S·ª≠ d·ª•ng ƒê·∫•t ƒê·ªìng Th√°p")
            
            # T·∫°o th∆∞ m·ª•c output
            output_dir = Path(self.config['paths']['output_dir'])
            output_dir.mkdir(exist_ok=True)
            (output_dir / 'maps').mkdir(exist_ok=True)
            (output_dir / 'reports').mkdir(exist_ok=True)
            (output_dir / 'statistics').mkdir(exist_ok=True)
            
            # Kh·ªüi t·∫°o preprocessor
            self.preprocessor = SatelliteDataPreprocessor(self.config_path)
            
            # Kh·ªüi t·∫°o change analyzer
            self.change_analyzer = LandUseChangeAnalyzer(self.config_path)
            
            # Kh·ªüi t·∫°o visualizer
            self.visualizer = MapVisualizer(self.config_path)
            
            self.logger.info("[SUCCESS] ƒê√£ kh·ªüi t·∫°o th√†nh c√¥ng c√°c components")
            
        except Exception as e:
            self.logger.error(f"[ERROR] L·ªói khi kh·ªüi t·∫°o components: {str(e)}")
            raise
    
    def initialize_gee(self):
        """Kh·ªüi t·∫°o Google Earth Engine"""
        try:
            self.logger.info("[GEE] ƒêang kh·ªüi t·∫°o Google Earth Engine...")
            self.gee_client = GEEClient(self.config_path)
            self.gee_client.set_study_area()
            self.logger.info("[SUCCESS] ƒê√£ kh·ªüi t·∫°o Google Earth Engine th√†nh c√¥ng")
        except Exception as e:
            self.logger.error(f"[ERROR] L·ªói khi kh·ªüi t·∫°o GEE: {str(e)}")
            raise
    
    def download_satellite_data(self):
        """T·∫£i d·ªØ li·ªáu ·∫£nh v·ªá tinh"""
        if self.gee_client is None:
            self.initialize_gee()
        
        self.logger.info("[DOWNLOAD] B·∫Øt ƒë·∫ßu t·∫£i d·ªØ li·ªáu ·∫£nh v·ªá tinh...")
        
        analysis_years = self.config['time_periods']['analysis_years']
        
        for year in analysis_years:
            try:
                self.logger.info(f"üì• T·∫£i d·ªØ li·ªáu nƒÉm {year}...")
                
                start_date = f"{year}-01-01"
                end_date = f"{year}-12-31"
                
                if year >= 2015:
                    # S·ª≠ d·ª•ng Sentinel-2
                    collection = self.gee_client.get_sentinel2_collection(start_date, end_date)
                else:
                    # S·ª≠ d·ª•ng Landsat
                    collection = self.gee_client.get_landsat_collection(start_date, end_date)
                
                # T·∫°o composite
                composite = self.gee_client.create_composite(collection, method='median')
                
                # Th√™m spectral indices
                composite_with_indices = self.gee_client.calculate_spectral_indices(composite)
                
                # Export to Drive
                task = self.gee_client.export_image_to_drive(
                    composite_with_indices,
                    f"dong_thap_{year}_composite",
                    scale=30 if year < 2015 else 10
                )
                
                self.logger.info(f"[SUCCESS] ƒê√£ kh·ªüi ƒë·ªông export cho nƒÉm {year}")
                
            except Exception as e:
                self.logger.error(f"[ERROR] L·ªói khi t·∫£i d·ªØ li·ªáu nƒÉm {year}: {str(e)}")
    
    def prepare_training_data(self, image_path: str, reference_path: str):
        """Chu·∫©n b·ªã d·ªØ li·ªáu training"""
        self.logger.info("üéØ Chu·∫©n b·ªã d·ªØ li·ªáu training...")
        
        try:
            # Tr√≠ch xu·∫•t training data
            features, labels = self.preprocessor.extract_training_data(
                image_path, reference_path
            )
            
            # Chu·∫©n h√≥a features
            features_normalized = self.preprocessor.normalize_data(features)
            
            self.logger.info(f"[SUCCESS] ƒê√£ chu·∫©n b·ªã {len(features)} m·∫´u training")
            return features_normalized, labels
            
        except Exception as e:
            self.logger.error(f"[ERROR] L·ªói khi chu·∫©n b·ªã training data: {str(e)}")
            raise
    
    def train_models(self, features, labels):
        """Hu·∫•n luy·ªán c√°c m√¥ h√¨nh ML"""
        self.logger.info("[TRAINING] B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán c√°c m√¥ h√¨nh ML...")
        
        try:
            # Random Forest
            self.logger.info("üå≤ Hu·∫•n luy·ªán Random Forest...")
            rf_model = RandomForestModel(self.config_path)
            rf_results = rf_model.train(features, labels)
            self.models['random_forest'] = rf_model
            
            # SVM
            self.logger.info("üéØ Hu·∫•n luy·ªán SVM...")
            svm_model = SVMModel(self.config_path)
            svm_results = svm_model.train(features, labels)
            self.models['svm'] = svm_model
            
            # CNN (n·∫øu c√≥ ƒë·ªß d·ªØ li·ªáu patch)
            if features.shape[0] > 1000:  # C·∫ßn ƒë·ªß d·ªØ li·ªáu cho CNN
                self.logger.info("üß† Hu·∫•n luy·ªán CNN...")
                cnn_model = CNNModel(self.config_path)
                # Reshape features cho CNN (c·∫ßn implement t√πy theo format)
                # cnn_results = cnn_model.train(features_reshaped, labels)
                # self.models['cnn'] = cnn_model
            
            # So s√°nh m√¥ h√¨nh
            evaluator = ModelEvaluator(self.config_path)
            # comparison = evaluator.compare_models(self.models, test_features, test_labels)
            
            self.logger.info("[SUCCESS] ƒê√£ ho√†n th√†nh hu·∫•n luy·ªán m√¥ h√¨nh")
            
        except Exception as e:
            self.logger.error(f"[ERROR] L·ªói khi hu·∫•n luy·ªán m√¥ h√¨nh: {str(e)}")
            raise
    
    def classify_images(self, image_paths: List[str]):
        """Ph√¢n lo·∫°i ·∫£nh s·ª≠ d·ª•ng m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán"""
        self.logger.info("[CLASSIFICATION] B·∫Øt ƒë·∫ßu ph√¢n lo·∫°i ·∫£nh...")
        
        if not self.models:
            self.logger.error("[ERROR] Ch∆∞a c√≥ m√¥ h√¨nh ƒë∆∞·ª£c hu·∫•n luy·ªán")
            return
        
        # S·ª≠ d·ª•ng m√¥ h√¨nh t·ªët nh·∫•t (Random Forest l√†m m·∫∑c ƒë·ªãnh)
        best_model = self.models.get('random_forest')
        if best_model is None:
            self.logger.error("[ERROR] Kh√¥ng t√¨m th·∫•y m√¥ h√¨nh ph√π h·ª£p")
            return
        
        classified_maps = {}
        
        for image_path in image_paths:
            try:
                year = self._extract_year_from_path(image_path)
                self.logger.info(f"üìä Ph√¢n lo·∫°i ·∫£nh nƒÉm {year}...")
                
                # ƒê·ªçc v√† ti·ªÅn x·ª≠ l√Ω ·∫£nh
                image_data, metadata = self.preprocessor.load_raster(image_path)
                
                # Reshape cho prediction
                height, width = image_data.shape[1], image_data.shape[2]
                features = image_data.reshape(image_data.shape[0], -1).T
                
                # Chu·∫©n h√≥a
                features_normalized = self.preprocessor.normalize_data(features, fit_scaler=False)
                
                # D·ª± ƒëo√°n
                predictions = best_model.predict(features_normalized)
                
                # Reshape v·ªÅ d·∫°ng ·∫£nh
                classified_map = predictions.reshape(height, width)
                
                # L∆∞u k·∫øt qu·∫£
                output_path = f"outputs/maps/classified_{year}.tif"
                self.preprocessor.save_processed_data(
                    classified_map[np.newaxis, :, :], output_path, metadata
                )
                
                classified_maps[year] = classified_map
                self.logger.info(f"[SUCCESS] ƒê√£ ph√¢n lo·∫°i v√† l∆∞u ·∫£nh nƒÉm {year}")
                
            except Exception as e:
                self.logger.error(f"[ERROR] L·ªói khi ph√¢n lo·∫°i ·∫£nh {image_path}: {str(e)}")
        
        return classified_maps
    
    def analyze_changes(self, classified_maps: Dict):
        """Ph√¢n t√≠ch bi·∫øn ƒë·ªông s·ª≠ d·ª•ng ƒë·∫•t"""
        self.logger.info("[ANALYSIS] B·∫Øt ƒë·∫ßu ph√¢n t√≠ch bi·∫øn ƒë·ªông...")
        
        try:
            analysis_results = {
                'area_statistics': {},
                'change_matrices': {},
                'change_rates': {},
                'landscape_metrics': {}
            }
            
            years = sorted(classified_maps.keys())
            pixel_size = 30 * 30  # 30m resolution
            
            # Th·ªëng k√™ di·ªán t√≠ch cho t·ª´ng nƒÉm
            for year in years:
                area_stats = self.change_analyzer.calculate_area_statistics(
                    classified_maps[year], pixel_size
                )
                analysis_results['area_statistics'][year] = area_stats
            
            # Ma tr·∫≠n bi·∫øn ƒë·ªông gi·ªØa c√°c k·ª≥
            change_periods = self.config['change_analysis']['change_periods']
            for period in change_periods:
                year1, year2 = int(period[0]), int(period[1])
                if year1 in classified_maps and year2 in classified_maps:
                    
                    change_matrix = self.change_analyzer.create_change_matrix(
                        classified_maps[year1], classified_maps[year2]
                    )
                    analysis_results['change_matrices'][f"{year1}-{year2}"] = change_matrix
                    
                    # T·ªëc ƒë·ªô thay ƒë·ªïi
                    change_rates = self.change_analyzer.calculate_change_rates(
                        analysis_results['area_statistics'][year1],
                        analysis_results['area_statistics'][year2],
                        year2 - year1
                    )
                    analysis_results['change_rates'][f"{year1}-{year2}"] = change_rates
            
            # Landscape metrics
            for year in years:
                metrics = self.change_analyzer.calculate_landscape_metrics(
                    classified_maps[year]
                )
                analysis_results['landscape_metrics'][year] = metrics
            
            self.logger.info("[SUCCESS] ƒê√£ ho√†n th√†nh ph√¢n t√≠ch bi·∫øn ƒë·ªông")
            return analysis_results
            
        except Exception as e:
            self.logger.error(f"[ERROR] L·ªói khi ph√¢n t√≠ch bi·∫øn ƒë·ªông: {str(e)}")
            raise
    
    def create_visualizations(self, classified_maps: Dict, analysis_results: Dict):
        """T·∫°o c√°c s·∫£n ph·∫©m tr·ª±c quan"""
        self.logger.info("[VISUALIZATION] T·∫°o c√°c s·∫£n ph·∫©m tr·ª±c quan...")
        
        try:
            years = sorted(classified_maps.keys())
            
            # B·∫£n ƒë·ªì ph√¢n lo·∫°i cho t·ª´ng nƒÉm
            for year in years:
                map_path = f"outputs/maps/classification_map_{year}.png"
                # Chuy·ªÉn ƒë·ªïi classified_map th√†nh raster file tr∆∞·ªõc khi visualize
                # self.visualizer.create_classification_map(
                #     f"outputs/maps/classified_{year}.tif", 
                #     map_path,
                #     f"B·∫£n ƒë·ªì s·ª≠ d·ª•ng ƒë·∫•t nƒÉm {year}"
                # )
            
            # B·∫£n ƒë·ªì bi·∫øn ƒë·ªông
            change_periods = self.config['change_analysis']['change_periods']
            for period in change_periods:
                year1, year2 = int(period[0]), int(period[1])
                if year1 in classified_maps and year2 in classified_maps:
                    change_map_path = f"outputs/maps/change_map_{year1}_{year2}.png"
                    # self.visualizer.create_change_map(
                    #     f"outputs/maps/classified_{year1}.tif",
                    #     f"outputs/maps/classified_{year2}.tif",
                    #     change_map_path,
                    #     f"Bi·∫øn ƒë·ªông s·ª≠ d·ª•ng ƒë·∫•t {year1}-{year2}"
                    # )
            
            # Bi·ªÉu ƒë·ªì di·ªán t√≠ch
            area_chart_path = "outputs/maps/area_trends.png"
            self.visualizer.create_area_chart(
                analysis_results['area_statistics'],
                area_chart_path,
                'line'
            )
            
            # Heatmap ma tr·∫≠n bi·∫øn ƒë·ªông
            for period, matrix in analysis_results['change_matrices'].items():
                heatmap_path = f"outputs/maps/change_matrix_{period}.png"
                self.visualizer.create_change_matrix_heatmap(
                    matrix, heatmap_path, f"Ma tr·∫≠n bi·∫øn ƒë·ªông {period}"
                )
            
            # Dashboard t∆∞∆°ng t√°c
            dashboard_path = "outputs/maps/dashboard.html"
            self.visualizer.create_plotly_dashboard(
                analysis_results, dashboard_path
            )
            
            self.logger.info("[SUCCESS] ƒê√£ t·∫°o xong c√°c s·∫£n ph·∫©m tr·ª±c quan")
            
        except Exception as e:
            self.logger.error(f"[ERROR] L·ªói khi t·∫°o tr·ª±c quan: {str(e)}")
    
    def generate_reports(self, analysis_results: Dict):
        """T·∫°o b√°o c√°o"""
        self.logger.info("[REPORT] T·∫°o b√°o c√°o...")
        
        try:
            # B√°o c√°o Excel
            excel_path = "outputs/reports/dong_thap_analysis_results.xlsx"
            self.change_analyzer.export_change_statistics(
                analysis_results, excel_path
            )
            
            # B√°o c√°o Markdown
            report_path = "outputs/reports/dong_thap_change_report.md"
            self.change_analyzer.generate_change_report(
                analysis_results, report_path
            )
            
            self.logger.info("[SUCCESS] ƒê√£ t·∫°o xong b√°o c√°o")
            
        except Exception as e:
            self.logger.error(f"[ERROR] L·ªói khi t·∫°o b√°o c√°o: {str(e)}")
    
    def _extract_year_from_path(self, path: str) -> int:
        """Tr√≠ch xu·∫•t nƒÉm t·ª´ ƒë∆∞·ªùng d·∫´n file"""
        import re
        match = re.search(r'(\d{4})', path)
        if match:
            return int(match.group(1))
        return 2020  # default
    
    def _create_sample_training_data(self):
        """T·∫°o d·ªØ li·ªáu hu·∫•n luy·ªán m·∫´u"""
        import numpy as np
        
        # T·∫°o d·ªØ li·ªáu m·∫´u v·ªõi 8 features (6 bands + 2 indices)
        n_samples = 5000
        n_features = 8
        
        # T·∫°o features v·ªõi ph√¢n ph·ªëi kh√°c nhau cho t·ª´ng l·ªõp
        features = []
        labels = []
        
        # Class 1: N√¥ng nghi·ªáp (NDVI cao, NIR cao)
        n_agri = 1500
        agri_features = np.random.normal([0.3, 0.4, 0.3, 0.7, 0.8, 0.2, 0.6, 0.1], 
                                       [0.1, 0.1, 0.1, 0.15, 0.1, 0.1, 0.1, 0.05], 
                                       (n_agri, n_features))
        features.extend(agri_features)
        labels.extend([1] * n_agri)
        
        # Class 2: ƒê√¥ th·ªã (NIR th·∫•p, ƒë·ªô ph·∫£n x·∫° cao)
        n_urban = 1000
        urban_features = np.random.normal([0.6, 0.6, 0.6, 0.4, 0.5, 0.4, 0.2, 0.3], 
                                        [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.05, 0.1], 
                                        (n_urban, n_features))
        features.extend(urban_features)
        labels.extend([2] * n_urban)
        
        # Class 3: R·ª´ng (NDVI r·∫•t cao, NIR cao)
        n_forest = 1000
        forest_features = np.random.normal([0.2, 0.3, 0.2, 0.8, 0.9, 0.1, 0.8, 0.0], 
                                         [0.05, 0.05, 0.05, 0.1, 0.05, 0.05, 0.05, 0.02], 
                                         (n_forest, n_features))
        features.extend(forest_features)
        labels.extend([3] * n_forest)
        
        # Class 4: M·∫∑t n∆∞·ªõc (NIR th·∫•p, MNDWI cao)
        n_water = 800
        water_features = np.random.normal([0.1, 0.2, 0.1, 0.05, 0.1, 0.05, -0.3, 0.8], 
                                        [0.05, 0.05, 0.05, 0.02, 0.05, 0.02, 0.1, 0.1], 
                                        (n_water, n_features))
        features.extend(water_features)
        labels.extend([4] * n_water)
        
        # Class 5: ƒê·∫•t tr·ªëng
        n_bare = 700
        bare_features = np.random.normal([0.5, 0.5, 0.4, 0.3, 0.4, 0.3, 0.1, -0.2], 
                                       [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.05, 0.1], 
                                       (n_bare, n_features))
        features.extend(bare_features)
        labels.extend([5] * n_bare)
        
        # Convert to numpy arrays
        features = np.array(features)
        labels = np.array(labels)
        
        # Tr·ªôn d·ªØ li·ªáu
        indices = np.random.permutation(len(features))
        features = features[indices]
        labels = labels[indices]
        
        return features, labels
    
    def _train_sample_model(self, features, labels):
        """Hu·∫•n luy·ªán m√¥ h√¨nh m·∫´u"""
        from src.models.ml_models import RandomForestModel, ModelEvaluator
        
        # T·∫°o v√† hu·∫•n luy·ªán m√¥ h√¨nh Random Forest
        rf_model = RandomForestModel(self.config_path)
        results = rf_model.train(features, labels)
        
        # Validation chi ti·∫øt n·∫øu ƒë∆∞·ª£c b·∫≠t
        if self.validation_mode:
            print("\nüîç VALIDATION CHI TI·∫æT M√î H√åNH:")
            print("-" * 60)
            
            evaluator = ModelEvaluator(self.config_path)
            
            # T·∫°o test data
            from sklearn.model_selection import train_test_split
            X_temp, X_test, y_temp, y_test = train_test_split(
                features, labels, test_size=0.2, random_state=42, stratify=labels
            )
            
            # Detailed metrics
            y_pred = rf_model.predict(X_test)
            detailed_metrics = evaluator.calculate_detailed_metrics(
                y_test, y_pred, 
                class_names=['ƒê·∫•t n√¥ng nghi·ªáp', 'ƒê·∫•t ƒë√¥ th·ªã', 'R·ª´ng', 'M·∫∑t n∆∞·ªõc', 'ƒê·∫•t tr·ªëng']
            )
            
            print(f"üìä Overall Accuracy: {detailed_metrics['overall_accuracy']:.3f}")
            print(f"üìä Kappa Coefficient: {detailed_metrics['kappa_coefficient']:.3f}")
            
            print("\nüìà Producer's Accuracy (Recall theo t·ª´ng l·ªõp):")
            class_names = ['ƒê·∫•t n√¥ng nghi·ªáp', 'ƒê·∫•t ƒë√¥ th·ªã', 'R·ª´ng', 'M·∫∑t n∆∞·ªõc', 'ƒê·∫•t tr·ªëng']
            for i, acc in enumerate(detailed_metrics['producers_accuracy']):
                print(f"   ‚Ä¢ {class_names[i]}: {acc:.3f}")
            
            print("\nüìà User's Accuracy (Precision theo t·ª´ng l·ªõp):")
            for i, acc in enumerate(detailed_metrics['users_accuracy']):
                print(f"   ‚Ä¢ {class_names[i]}: {acc:.3f}")
            
            # Th√™m ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng model
            self._validate_model_quality(detailed_metrics)
        
        # L∆∞u m√¥ h√¨nh
        rf_model.save_model("outputs/sample_rf_model.pkl")
        
        return results
    
    def _validate_model_quality(self, metrics):
        """ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng m√¥ h√¨nh"""
        print(f"\n‚úÖ ƒê√ÅNH GI√Å CH·∫§T L∆Ø·ª¢NG M√î H√åNH:")
        print("-" * 60)
        
        accuracy = metrics['overall_accuracy']
        kappa = metrics['kappa_coefficient']
        
        # ƒê√°nh gi√° overall accuracy
        if accuracy >= 0.95:
            acc_status = "üü¢ XU·∫§T S·∫ÆC"
        elif accuracy >= 0.90:
            acc_status = "üü° T·ªêT" 
        elif accuracy >= 0.85:
            acc_status = "üü† CH·∫§P NH·∫¨N ƒê∆Ø·ª¢C"
        else:
            acc_status = "üî¥ C·∫¶N C·∫¢I THI·ªÜN"
            
        print(f"ƒê·ªô ch√≠nh x√°c: {accuracy:.1%} - {acc_status}")
        
        # ƒê√°nh gi√° Kappa coefficient
        if kappa >= 0.8:
            kappa_status = "üü¢ R·∫§T T·ªêT"
        elif kappa >= 0.6:
            kappa_status = "üü° T·ªêT"
        elif kappa >= 0.4:
            kappa_status = "üü† V·ª™A PH·∫¢I"
        else:
            kappa_status = "üî¥ Y·∫æU"
            
        print(f"H·ªá s·ªë Kappa: {kappa:.3f} - {kappa_status}")
        
        # ƒê√°nh gi√° ƒë·ªô c√¢n b·∫±ng gi·ªØa c√°c l·ªõp
        producer_acc = metrics['producers_accuracy']
        min_acc = min(producer_acc)
        max_acc = max(producer_acc)
        
        if max_acc - min_acc < 0.1:
            balance_status = "üü¢ C√ÇN B·∫∞NG T·ªêT"
        elif max_acc - min_acc < 0.2:
            balance_status = "üü° C√ÇN B·∫∞NG V·ª™A"
        else:
            balance_status = "üî¥ M·∫§T C√ÇN B·∫∞NG"
            
        print(f"C√¢n b·∫±ng l·ªõp: {balance_status} (ch√™nh l·ªách: {max_acc-min_acc:.3f})")
        
        # Khuy·∫øn ngh·ªã
        print(f"\nüí° KHUY·∫æN NGH·ªä:")
        if accuracy < 0.85:
            print("   ‚Ä¢ C·∫ßn th√™m d·ªØ li·ªáu training v√† ƒëi·ªÅu ch·ªânh parameters")
        if kappa < 0.6:
            print("   ‚Ä¢ C·∫ßn c·∫£i thi·ªán ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu ground truth")
        if max_acc - min_acc > 0.2:
            print("   ‚Ä¢ C·∫ßn c√¢n b·∫±ng s·ªë l∆∞·ª£ng m·∫´u gi·ªØa c√°c l·ªõp")
        if accuracy >= 0.95 and kappa >= 0.8:
            print("   ‚Ä¢ M√¥ h√¨nh ƒë·∫°t ch·∫•t l∆∞·ª£ng cao, s·∫µn s√†ng ·ª©ng d·ª•ng th·ª±c t·∫ø")
    
    def _create_sample_classified_maps(self):
        """T·∫°o b·∫£n ƒë·ªì ph√¢n lo·∫°i m·∫´u"""
        import numpy as np
        
        height, width = 100, 100
        # L·∫•y t·ª´ config ƒë·ªÉ ph√¢n t√≠ch ƒë·∫ßy ƒë·ªß ƒë·∫øn 2025
        years = self.config['time_periods']['analysis_years']
        classified_maps = {}
        
        np.random.seed(42)
        for i, year in enumerate(years):
            # T·∫°o xu h∆∞·ªõng ƒë√¥ th·ªã h√≥a m·∫°nh m·∫Ω theo th·ªùi gian
            if year <= 2000:
                # Giai ƒëo·∫°n 1990-2000: ƒê√¥ th·ªã h√≥a ch·∫≠m
                probabilities = [0.45, 0.08, 0.22, 0.20, 0.05]
            elif year <= 2010:
                # Giai ƒëo·∫°n 2000-2010: ƒê√¥ th·ªã h√≥a tƒÉng t·ªëc
                probabilities = [0.35, 0.15, 0.20, 0.22, 0.08]
            elif year <= 2020:
                # Giai ƒëo·∫°n 2010-2020: ƒê√¥ th·ªã h√≥a m·∫°nh
                probabilities = [0.25, 0.25, 0.20, 0.22, 0.08]
            else:
                # Giai ƒëo·∫°n 2020-2025: ƒê√¥ th·ªã h√≥a r·∫•t m·∫°nh
                probabilities = [0.20, 0.32, 0.18, 0.22, 0.08]
            
            base_map = np.random.choice([1, 2, 3, 4, 5], size=(height, width), p=probabilities)
            classified_maps[year] = base_map
            
        return classified_maps
    
    def _print_analysis_summary(self, analysis_results):
        """In t√≥m t·∫Øt k·∫øt qu·∫£ ph√¢n t√≠ch"""
        print("\n" + "="*80)
        print("üìä B√ÅO C√ÅO PH√ÇN T√çCH BI·∫æN ƒê·ªòNG S·ª¨ D·ª§NG ƒê·∫§T T·ªàNH ƒê·ªíNG TH√ÅP")
        print("üåæ Giai ƒëo·∫°n: 1990 - 2025 | ƒê∆°n v·ªã: hecta (ha) | ƒê·ªô ph√¢n gi·∫£i: 30m")
        print("="*80)
        
        # Th·ªëng k√™ di·ªán t√≠ch v·ªõi format ƒë·∫πp
        if 'area_statistics' in analysis_results:
            print("\nüìà DI·ªÑN BI·∫æN DI·ªÜN T√çCH C√ÅC LO·∫†I ƒê·∫§T THEO TH·ªúI GIAN")
            print("-" * 80)
            
            # T·∫°o b·∫£ng so s√°nh
            all_years = sorted(analysis_results['area_statistics'].keys())
            all_classes = analysis_results['area_statistics'][all_years[0]]['Class_Name'].unique()
            
            # Header
            print(f"{'Lo·∫°i ƒë·∫•t':<20}", end="")
            for year in all_years:
                print(f"{year:>12}", end="")
            print()
            print("-" * 80)
            
            # Data rows
            for class_name in all_classes:
                print(f"{class_name:<20}", end="")
                for year in all_years:
                    stats = analysis_results['area_statistics'][year]
                    area = stats[stats['Class_Name'] == class_name]['Area_ha'].iloc[0]
                    pct = stats[stats['Class_Name'] == class_name]['Percentage'].iloc[0]
                    print(f"{area:>8.1f}ha", end="")
                    print(f"({pct:>4.1f}%)", end="")
                print()
        
        # T·ªëc ƒë·ªô thay ƒë·ªïi v·ªõi ph√¢n t√≠ch chi ti·∫øt
        if 'change_rates' in analysis_results:
            print(f"\nüîÑ T·ªêC ƒê·ªò THAY ƒê·ªîI H√ÄNG NƒÇM (%/nƒÉm)")
            print("-" * 80)
            
            for period, rates in analysis_results['change_rates'].items():
                years = period.split('-')
                duration = int(years[1]) - int(years[0])
                
                print(f"\nüìä Giai ƒëo·∫°n {period} ({duration} nƒÉm):")
                print(f"{'Lo·∫°i ƒë·∫•t':<20} {'Thay ƒë·ªïi':<15} {'T·ªëc ƒë·ªô':<12} {'ƒê√°nh gi√°':<15}")
                print("-" * 70)
                
                for _, row in rates.iterrows():
                    change = row['Change_ha']
                    rate = row['Annual_change_rate']
                    
                    # ƒê√°nh gi√° m·ª©c ƒë·ªô thay ƒë·ªïi
                    if abs(rate) < 0.5:
                        assessment = "·ªîn ƒë·ªãnh"
                    elif abs(rate) < 2.0:
                        assessment = "Thay ƒë·ªïi v·ª´a"
                    else:
                        assessment = "Thay ƒë·ªïi m·∫°nh"
                    
                    trend = "tƒÉng" if rate > 0 else "gi·∫£m" if rate < 0 else "kh√¥ng ƒë·ªïi"
                    
                    print(f"{row['Class_Name']:<20} "
                          f"{change:>+8.1f}ha     "
                          f"{trend} {abs(rate):>4.1f}%/nƒÉm "
                          f"{assessment:<15}")
        
        # Nh·∫≠n x√©t chi ti·∫øt v√† khuy·∫øn ngh·ªã
        self._print_detailed_insights(analysis_results)
    
    def _print_detailed_insights(self, analysis_results):
        """In nh·∫≠n x√©t chi ti·∫øt v√† khuy·∫øn ngh·ªã"""
        print(f"\nüí° NH·∫¨N X√âT V√Ä PH√ÇN T√çCH CHI TI·∫æT")
        print("-" * 80)
        
        if 'change_rates' in analysis_results:
            # Ph√¢n t√≠ch xu h∆∞·ªõng t·ªïng th·ªÉ
            latest_period = list(analysis_results['change_rates'].keys())[-1]
            latest_rates = analysis_results['change_rates'][latest_period]
            
            print("\nüåÜ XU H∆Ø·ªöNG ƒê√î TH·ªä H√ìA:")
            urban_rate = latest_rates[latest_rates['Class_Name'] == 'ƒê·∫•t ƒë√¥ th·ªã']['Annual_change_rate'].iloc[0]
            if urban_rate > 2:
                print(f"   ‚Ä¢ ƒê√¥ th·ªã h√≥a M·∫†NH v·ªõi t·ªëc ƒë·ªô {urban_rate:.1f}%/nƒÉm")
                print("   ‚Ä¢ C·∫ßn quy ho·∫°ch k·ªπ l∆∞·ª°ng ƒë·ªÉ ƒë·∫£m b·∫£o ph√°t tri·ªÉn b·ªÅn v·ªØng")
            elif urban_rate > 1:
                print(f"   ‚Ä¢ ƒê√¥ th·ªã h√≥a V·ª™A PH·∫¢I v·ªõi t·ªëc ƒë·ªô {urban_rate:.1f}%/nƒÉm")
                print("   ‚Ä¢ Xu h∆∞·ªõng ph√°t tri·ªÉn ·ªïn ƒë·ªãnh")
            
            print("\nüåæ BI·∫æN ƒê·ªòNG N√îNG NGHI·ªÜP:")
            agri_rate = latest_rates[latest_rates['Class_Name'] == 'ƒê·∫•t n√¥ng nghi·ªáp']['Annual_change_rate'].iloc[0]
            if agri_rate < -1:
                print(f"   ‚Ä¢ Di·ªán t√≠ch n√¥ng nghi·ªáp GI·∫¢M v·ªõi t·ªëc ƒë·ªô {abs(agri_rate):.1f}%/nƒÉm")
                print("   ‚Ä¢ C·∫ßn c√¢n b·∫±ng gi·ªØa ph√°t tri·ªÉn ƒë√¥ th·ªã v√† an ninh l∆∞∆°ng th·ª±c")
            
            print("\nüå≥ B·∫¢O T·ªíN T√ÄI NGUY√äN:")
            forest_rate = latest_rates[latest_rates['Class_Name'] == 'R·ª´ng']['Annual_change_rate'].iloc[0]
            water_rate = latest_rates[latest_rates['Class_Name'] == 'M·∫∑t n∆∞·ªõc']['Annual_change_rate'].iloc[0]
            
            if abs(forest_rate) < 0.5 and abs(water_rate) < 0.5:
                print("   ‚Ä¢ T√†i nguy√™n r·ª´ng v√† n∆∞·ªõc ƒë∆∞·ª£c b·∫£o t·ªìn T·ªêT")
                print("   ‚Ä¢ C·∫ßn duy tr√¨ ch√≠nh s√°ch b·∫£o v·ªá m√¥i tr∆∞·ªùng")
        
        print(f"\nüéØ KHUY·∫æN NGH·ªä CH√çNH S√ÅCH:")
        print("   1. üèòÔ∏è  Quy ho·∫°ch ƒë√¥ th·ªã th√¥ng minh ƒë·ªÉ ki·ªÉm so√°t ƒë√¥ th·ªã h√≥a")
        print("   2. üåæ B·∫£o v·ªá ƒë·∫•t n√¥ng nghi·ªáp ch·∫•t l∆∞·ª£ng cao")
        print("   3. üå≥ Duy tr√¨ ƒë·ªô che ph·ªß r·ª´ng t·ªëi thi·ªÉu 20%")
        print("   4. üíß B·∫£o v·ªá ngu·ªìn n∆∞·ªõc v√† h·ªá sinh th√°i ven s√¥ng")
        print("   5. üìä Gi√°m s√°t ƒë·ªãnh k·ª≥ b·∫±ng c√¥ng ngh·ªá vi·ªÖn th√°m")
        
        print(f"\n‚ö†Ô∏è  L∆ØU √ù QUAN TR·ªåNG:")
        print("   ‚Ä¢ K·∫øt qu·∫£ n√†y l√† m√¥ ph·ªèng demo v·ªõi d·ªØ li·ªáu m·∫´u")
        print("   ‚Ä¢ ƒê·ªÉ c√≥ k·∫øt qu·∫£ ch√≠nh x√°c, c·∫ßn s·ª≠ d·ª•ng d·ªØ li·ªáu th·ª±c t·ª´ Google Earth Engine")
        print("   ‚Ä¢ ƒê·ªô tin c·∫≠y ph·ª• thu·ªôc v√†o ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu ground truth")
    
    def run_full_analysis(self):
        """Ch·∫°y to√†n b·ªô quy tr√¨nh ph√¢n t√≠ch"""
        self.logger.info("[FULL_ANALYSIS] B·∫Øt ƒë·∫ßu quy tr√¨nh ph√¢n t√≠ch ho√†n ch·ªânh...")
        
        try:
            # B∆∞·ªõc 1: T·∫£i d·ªØ li·ªáu v·ªá tinh
            print("\n[STEP 1] T·∫£i d·ªØ li·ªáu ·∫£nh v·ªá tinh t·ª´ Google Earth Engine")
            self.download_satellite_data()
            
            print("\n‚è≥ Ch·ªù Google Earth Engine x·ª≠ l√Ω d·ªØ li·ªáu...")
            print("üìã Trong l√∫c ch·ªù, s·∫Ω ch·∫°y workflow demo v·ªõi d·ªØ li·ªáu m·∫´u:")
            
            # Ch·∫°y demo analysis v·ªõi d·ªØ li·ªáu m·∫´u ƒë·ªÉ minh h·ªça quy tr√¨nh
            print("\n" + "="*60)
            print("üé≠ CH·∫†Y DEMO WORKFLOW V·ªöI D·ªÆ LI·ªÜU M·∫™U")
            print("="*60)
            
            # B∆∞·ªõc 2: T·∫°o d·ªØ li·ªáu m·∫´u
            print("\n[DEMO STEP 2] T·∫°o d·ªØ li·ªáu hu·∫•n luy·ªán m·∫´u...")
            features, labels = self._create_sample_training_data()
            print(f"[SUCCESS] ƒê√£ t·∫°o {len(features)} m·∫´u v·ªõi {features.shape[1]} ƒë·∫∑c tr∆∞ng")
            
            # B∆∞·ªõc 3: Hu·∫•n luy·ªán m√¥ h√¨nh
            print("\n[DEMO STEP 3] Hu·∫•n luy·ªán m√¥ h√¨nh Random Forest...")
            model_results = self._train_sample_model(features, labels)
            print(f"[SUCCESS] ƒê·ªô ch√≠nh x√°c m√¥ h√¨nh: {model_results['accuracy']:.3f}")
            
            # B∆∞·ªõc 4: T·∫°o b·∫£n ƒë·ªì ph√¢n lo·∫°i m·∫´u
            print("\n[DEMO STEP 4] T·∫°o b·∫£n ƒë·ªì ph√¢n lo·∫°i m·∫´u...")
            classified_maps = self._create_sample_classified_maps()
            print(f"[SUCCESS] ƒê√£ t·∫°o {len(classified_maps)} b·∫£n ƒë·ªì m·∫´u")
            
            # B∆∞·ªõc 5: Ph√¢n t√≠ch bi·∫øn ƒë·ªông
            print("\n[DEMO STEP 5] Ph√¢n t√≠ch bi·∫øn ƒë·ªông...")
            analysis_results = self.analyze_changes(classified_maps)
            print("[SUCCESS] ƒê√£ ho√†n th√†nh ph√¢n t√≠ch bi·∫øn ƒë·ªông")
            
            # B∆∞·ªõc 6: T·∫°o tr·ª±c quan
            print("\n[DEMO STEP 6] T·∫°o s·∫£n ph·∫©m tr·ª±c quan...")
            self.create_visualizations(classified_maps, analysis_results)
            print("[SUCCESS] ƒê√£ t·∫°o c√°c s·∫£n ph·∫©m tr·ª±c quan")
            
            # B∆∞·ªõc 7: T·∫°o b√°o c√°o
            print("\n[DEMO STEP 7] T·∫°o b√°o c√°o...")
            self.generate_reports(analysis_results)
            print("[SUCCESS] ƒê√£ t·∫°o b√°o c√°o chi ti·∫øt")
            
            # T√≥m t·∫Øt k·∫øt qu·∫£
            self._print_analysis_summary(analysis_results)
            
            self.logger.info("[COMPLETE] Ho√†n th√†nh workflow demo!")
            
            print("\n" + "="*60)
            print("üéØ K·∫æT QU·∫¢ WORKFLOW")
            print("="*60)
            print("‚úÖ Demo workflow ho√†n th√†nh th√†nh c√¥ng!")
            print("üìä D·ªØ li·ªáu t·ª´ Google Earth Engine ƒëang ƒë∆∞·ª£c x·ª≠ l√Ω song song")
            print("\nüí° Ki·ªÉm tra ti·∫øn ƒë·ªô GEE: python scripts/check_gee_tasks.py")
            print("üìÇ K·∫øt qu·∫£ demo l∆∞u t·∫°i: outputs/")
            
        except Exception as e:
            self.logger.error(f"[ERROR] L·ªói trong quy tr√¨nh ph√¢n t√≠ch: {str(e)}")
            raise
    
    def run_demo_analysis(self):
        """Ch·∫°y demo ph√¢n t√≠ch v·ªõi d·ªØ li·ªáu m·∫´u (kh√¥ng c·∫ßn GEE)"""
        self.logger.info("[DEMO] B·∫Øt ƒë·∫ßu demo ph√¢n t√≠ch v·ªõi d·ªØ li·ªáu m·∫´u...")
        
        try:
            import numpy as np
            
            print("\n" + "="*60)
            print("üåæ DEMO PH√ÇN T√çCH BI·∫æN ƒê·ªòNG S·ª¨ D·ª§NG ƒê·∫§T üåæ")
            print("="*60)
            print("Ch·∫ø ƒë·ªô demo s·ª≠ d·ª•ng d·ªØ li·ªáu m·∫´u ƒë·ªÉ minh h·ªça quy tr√¨nh")
            print("="*60)
            
            # T·∫°o d·ªØ li·ªáu m·∫´u
            print("\n[DEMO STEP 1] T·∫°o d·ªØ li·ªáu m·∫´u...")
            height, width = 100, 100
            years = [1990, 2000, 2010, 2020]
            classified_maps = {}
            
            # T·∫°o maps gi·∫£ l·∫≠p v·ªõi xu h∆∞·ªõng ƒë√¥ th·ªã h√≥a
            np.random.seed(42)
            for i, year in enumerate(years):
                # T·∫°o b·∫£n ƒë·ªì v·ªõi xu h∆∞·ªõng thay ƒë·ªïi theo th·ªùi gian
                # Class 1: N√¥ng nghi·ªáp (gi·∫£m d·∫ßn)
                # Class 2: ƒê√¥ th·ªã (tƒÉng d·∫ßn) 
                # Class 3: R·ª´ng (·ªïn ƒë·ªãnh)
                # Class 4: M·∫∑t n∆∞·ªõc (·ªïn ƒë·ªãnh)
                # Class 5: ƒê·∫•t tr·ªëng (gi·∫£m)
                base_map = np.random.choice(
                    [1, 2, 3, 4, 5], 
                    size=(height, width),
                    p=[0.4-i*0.05, 0.1+i*0.05, 0.2, 0.2, 0.1]
                )
                classified_maps[year] = base_map
            
            print(f"[SUCCESS] ƒê√£ t·∫°o {len(classified_maps)} b·∫£n ƒë·ªì m·∫´u")
            
            # Ph√¢n t√≠ch th·ªëng k√™
            print("\n[DEMO STEP 2] Ph√¢n t√≠ch th·ªëng k√™ di·ªán t√≠ch...")
            pixel_size = 30 * 30  # 30m resolution
            area_statistics = {}
            
            for year, classified_map in classified_maps.items():
                area_stats = self.change_analyzer.calculate_area_statistics(
                    classified_map, pixel_size
                )
                area_statistics[year] = area_stats
                print(f"NƒÉm {year}: {len(area_stats)} lo·∫°i ƒë·∫•t ƒë∆∞·ª£c ph√¢n t√≠ch")
            
            # T·∫°o ma tr·∫≠n bi·∫øn ƒë·ªông
            print("\n[DEMO STEP 3] T√≠nh ma tr·∫≠n bi·∫øn ƒë·ªông...")
            change_matrix = self.change_analyzer.create_change_matrix(
                classified_maps[1990], classified_maps[2020]
            )
            print(f"Ma tr·∫≠n bi·∫øn ƒë·ªông 1990-2020: {change_matrix.shape}")
            
            # T√≠nh t·ªëc ƒë·ªô thay ƒë·ªïi
            change_rates = self.change_analyzer.calculate_change_rates(
                area_statistics[1990], area_statistics[2020], 30
            )
            print("T·ªëc ƒë·ªô thay ƒë·ªïi h√†ng nƒÉm ƒë√£ ƒë∆∞·ª£c t√≠nh to√°n")
            
            # T·∫°o tr·ª±c quan
            print("\n[DEMO STEP 4] T·∫°o s·∫£n ph·∫©m tr·ª±c quan...")
            
            # Bi·ªÉu ƒë·ªì xu h∆∞·ªõng
            area_chart_path = "outputs/demo_area_trends.png"
            self.visualizer.create_area_chart(
                area_statistics, area_chart_path, 'line'
            )
            print(f"[SUCCESS] ƒê√£ t·∫°o bi·ªÉu ƒë·ªì: {area_chart_path}")
            
            # Ma tr·∫≠n bi·∫øn ƒë·ªông
            matrix_path = "outputs/demo_change_matrix.png"  
            self.visualizer.create_change_matrix_heatmap(
                change_matrix, matrix_path, "Ma tr·∫≠n bi·∫øn ƒë·ªông Demo 1990-2020"
            )
            print(f"[SUCCESS] ƒê√£ t·∫°o heatmap: {matrix_path}")
            
            # T·∫°o b√°o c√°o demo
            print("\n[DEMO STEP 5] T·∫°o b√°o c√°o demo...")
            analysis_results = {
                'area_statistics': area_statistics,
                'change_matrices': {'1990-2020': change_matrix},
                'change_rates': {'1990-2020': change_rates}
            }
            
            report_path = "outputs/demo_report.md"
            self.change_analyzer.generate_change_report(
                analysis_results, report_path
            )
            print(f"[SUCCESS] ƒê√£ t·∫°o b√°o c√°o: {report_path}")
            
            # T√≥m t·∫Øt k·∫øt qu·∫£
            print("\n" + "="*60)
            print("üìä K·∫æT QU·∫¢ DEMO")
            print("="*60)
            
            print("\nüîç Xu h∆∞·ªõng ch√≠nh ƒë∆∞·ª£c ph√°t hi·ªán:")
            for _, row in change_rates.iterrows():
                if abs(row['Annual_change_rate']) > 0.5:
                    trend = "tƒÉng" if row['Annual_change_rate'] > 0 else "gi·∫£m"
                    print(f"  ‚Ä¢ {row['Class_Name']}: {trend} {abs(row['Annual_change_rate']):.1f}%/nƒÉm")
            
            print(f"\nüìÇ S·∫£n ph·∫©m ƒë√£ t·∫°o:")
            print(f"  ‚Ä¢ Bi·ªÉu ƒë·ªì xu h∆∞·ªõng: {area_chart_path}")
            print(f"  ‚Ä¢ Ma tr·∫≠n bi·∫øn ƒë·ªông: {matrix_path}")
            print(f"  ‚Ä¢ B√°o c√°o chi ti·∫øt: {report_path}")
            
            print("\n‚ú® Demo ho√†n th√†nh th√†nh c√¥ng!")
            print("\nƒê·ªÉ ch·∫°y v·ªõi d·ªØ li·ªáu th·ª±c:")
            print("1. Thi·∫øt l·∫≠p Google Earth Engine: python scripts/setup_gee.py")
            print("2. Ch·∫°y ph√¢n t√≠ch ƒë·∫ßy ƒë·ªß: python main.py --mode full")
            
            self.logger.info("[DEMO SUCCESS] Demo ho√†n th√†nh th√†nh c√¥ng!")
            
        except Exception as e:
            self.logger.error(f"[DEMO ERROR] L·ªói trong demo: {str(e)}")
            print(f"\n[ERROR] L·ªói demo: {str(e)}")
            raise

def main():
    """H√†m main"""
    parser = argparse.ArgumentParser(
        description="·ª®ng d·ª•ng ph√¢n t√≠ch bi·∫øn ƒë·ªông s·ª≠ d·ª•ng ƒë·∫•t t·ªânh ƒê·ªìng Th√°p"
    )
    
    parser.add_argument(
        '--config', 
        default='config/config.yaml',
        help='ƒê∆∞·ªùng d·∫´n file c·∫•u h√¨nh'
    )
    
    parser.add_argument(
        '--mode',
        choices=['download', 'train', 'classify', 'analyze', 'visualize', 'full', 'demo'],
        default='full',
        help='Ch·∫ø ƒë·ªô ch·∫°y ·ª©ng d·ª•ng'
    )
    
    parser.add_argument(
        '--input-dir',
        help='Th∆∞ m·ª•c ch·ª©a d·ªØ li·ªáu ƒë·∫ßu v√†o'
    )
    
    parser.add_argument(
        '--output-dir',
        help='Th∆∞ m·ª•c l∆∞u k·∫øt qu·∫£'
    )
    
    parser.add_argument(
        '--years',
        nargs='+',
        type=int,
        help='Danh s√°ch nƒÉm c·∫ßn ph√¢n t√≠ch (v√≠ d·ª•: --years 1990 2000 2010 2020 2025)'
    )
    
    parser.add_argument(
        '--start-year',
        type=int,
        default=1990,
        help='NƒÉm b·∫Øt ƒë·∫ßu ph√¢n t√≠ch (m·∫∑c ƒë·ªãnh: 1990)'
    )
    
    parser.add_argument(
        '--end-year', 
        type=int,
        default=2025,
        help='NƒÉm k·∫øt th√∫c ph√¢n t√≠ch (m·∫∑c ƒë·ªãnh: 2025)'
    )
    
    parser.add_argument(
        '--validation',
        action='store_true',
        help='B·∫≠t ch·∫ø ƒë·ªô validation chi ti·∫øt v·ªõi metrics ƒë√°nh gi√°'
    )
    
    args = parser.parse_args()
    
    try:
        # Kh·ªüi t·∫°o ·ª©ng d·ª•ng
        app = DongThapLandChangeApp(args.config)
        
        # C·∫≠p nh·∫≠t c·∫•u h√¨nh v·ªõi tham s·ªë t·ª´ command line
        if args.years:
            app.config['time_periods']['analysis_years'] = sorted(args.years)
            print(f"üóìÔ∏è  S·ª≠ d·ª•ng nƒÉm t√πy ch·ªânh: {args.years}")
        elif args.start_year or args.end_year:
            # T·∫°o danh s√°ch nƒÉm v·ªõi kho·∫£ng c√°ch 5 nƒÉm
            years = list(range(args.start_year, args.end_year + 1, 
                             app.config['time_periods']['min_year_gap']))
            if args.end_year not in years:
                years.append(args.end_year)
            app.config['time_periods']['analysis_years'] = years
            print(f"üóìÔ∏è  Ph√¢n t√≠ch t·ª´ {args.start_year} ƒë·∫øn {args.end_year}: {years}")
        
        # Thi·∫øt l·∫≠p validation mode
        app.validation_mode = args.validation
        if args.validation:
            print("üîç Ch·∫ø ƒë·ªô validation CHI TI·∫æT ƒë∆∞·ª£c k√≠ch ho·∫°t")
        
        print("=" * 80)
        print("üåæ PH√ÇN T√çCH BI·∫æN ƒê·ªòNG S·ª¨ D·ª§NG ƒê·∫§T T·ªàNH ƒê·ªíNG TH√ÅP üåæ")
        print(f"üìÖ Giai ƒëo·∫°n: {min(app.config['time_periods']['analysis_years'])} - {max(app.config['time_periods']['analysis_years'])}")
        print(f"üéØ Ch·∫ø ƒë·ªô: {args.mode.upper()}")
        print("=" * 80)
        
        if args.mode == 'download':
            app.download_satellite_data()
        elif args.mode == 'full':
            app.run_full_analysis()
        elif args.mode == 'demo':
            app.run_demo_analysis()
        else:
            print(f"Ch·∫ø ƒë·ªô {args.mode} ƒëang ƒë∆∞·ª£c ph√°t tri·ªÉn...")
        
        print("\n[SUCCESS] ·ª®ng d·ª•ng ho√†n th√†nh th√†nh c√¥ng!")
        
    except KeyboardInterrupt:
        print("\n[STOPPED] ·ª®ng d·ª•ng b·ªã d·ª´ng b·ªüi ng∆∞·ªùi d√πng")
    except Exception as e:
        print(f"\n[ERROR] L·ªói: {str(e)}")
        sys.exit(1)

if __name__ == "__main__":
    main()
